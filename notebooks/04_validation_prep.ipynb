{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da021525",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas numpy scipy scikit-learn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbe608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quality filtered dataset\n",
    "DATASET_PATH = '/content/drive/MyDrive/HIN_SIN/dataset/quality_filtered.csv'\n",
    "# Or for local: DATASET_PATH = '../dataset/quality_filtered.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\\n{df['Label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6505d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling strategy for human validation\n",
    "# Target: 1,000-1,500 samples with stratified sampling\n",
    "\n",
    "VALIDATION_SAMPLE_SIZE = 1000  # Adjust as needed (1000-1500)\n",
    "\n",
    "# Stratified sampling by label and quality flag\n",
    "def stratified_sample(df, n_samples, stratify_cols=['Label', 'quality_flag']):\n",
    "    \"\"\"\n",
    "    Perform stratified sampling to maintain distribution.\n",
    "    \"\"\"\n",
    "    # Calculate proportions\n",
    "    total = len(df)\n",
    "    sampled_dfs = []\n",
    "    \n",
    "    for label in df['Label'].unique():\n",
    "        for quality in df['quality_flag'].unique():\n",
    "            subset = df[(df['Label'] == label) & (df['quality_flag'] == quality)]\n",
    "            proportion = len(subset) / total\n",
    "            n_subset = max(1, int(n_samples * proportion))\n",
    "            \n",
    "            if len(subset) > 0:\n",
    "                sampled = subset.sample(n=min(n_subset, len(subset)), random_state=42)\n",
    "                sampled_dfs.append(sampled)\n",
    "    \n",
    "    result = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    \n",
    "    # Shuffle\n",
    "    result = result.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Check if quality_flag exists\n",
    "if 'quality_flag' not in df.columns:\n",
    "    # Simple random stratified sampling by label\n",
    "    validation_sample = df.groupby('Label', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min(VALIDATION_SAMPLE_SIZE//2, len(x)), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "else:\n",
    "    validation_sample = stratified_sample(df, VALIDATION_SAMPLE_SIZE)\n",
    "\n",
    "print(f\"Validation sample size: {len(validation_sample)}\")\n",
    "print(f\"\\nLabel distribution in sample:\\n{validation_sample['Label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5956556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotation template\n",
    "# Columns for annotators to fill:\n",
    "# - Label_Annotator1: 0/1\n",
    "# - Label_Annotator2: 0/1\n",
    "# - Intent_Preserved: Yes/No/Unclear\n",
    "# - CodeMix_Natural: Yes/No/Unclear\n",
    "# - Bullying_Strength: Same/Stronger/Weaker\n",
    "# - Comments: Free text\n",
    "\n",
    "annotation_template = validation_sample.copy()\n",
    "\n",
    "# Add annotation columns\n",
    "annotation_template['Label_Annotator1'] = ''\n",
    "annotation_template['Label_Annotator2'] = ''\n",
    "annotation_template['Intent_Preserved'] = ''  # Yes/No/Unclear\n",
    "annotation_template['CodeMix_Natural'] = ''   # Yes/No/Unclear  \n",
    "annotation_template['Bullying_Strength'] = '' # Same/Stronger/Weaker\n",
    "annotation_template['Comments'] = ''\n",
    "\n",
    "# Reorder columns for better annotation experience\n",
    "column_order = [\n",
    "    'ID',\n",
    "    'Text_HindiEnglish',\n",
    "    'Text_SinhalaEnglish', \n",
    "    'Label',  # Ground truth (hidden during annotation)\n",
    "    'Label_Annotator1',\n",
    "    'Label_Annotator2',\n",
    "    'Intent_Preserved',\n",
    "    'CodeMix_Natural',\n",
    "    'Bullying_Strength',\n",
    "    'Comments'\n",
    "]\n",
    "\n",
    "# Only include columns that exist\n",
    "available_cols = [c for c in column_order if c in annotation_template.columns]\n",
    "annotation_template = annotation_template[available_cols]\n",
    "\n",
    "print(\"Annotation template columns:\")\n",
    "print(annotation_template.columns.tolist())\n",
    "annotation_template.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa74bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save annotation template\n",
    "# CSV for Google Sheets import\n",
    "CSV_PATH = '/content/drive/MyDrive/HIN_SIN/annotations/annotation_template.csv'\n",
    "annotation_template.to_csv(CSV_PATH, index=False, encoding='utf-8')\n",
    "print(f\"Saved CSV template: {CSV_PATH}\")\n",
    "\n",
    "# Excel for easier annotation\n",
    "EXCEL_PATH = '/content/drive/MyDrive/HIN_SIN/annotations/annotation_template.xlsx'\n",
    "annotation_template.to_excel(EXCEL_PATH, index=False, engine='openpyxl')\n",
    "print(f\"Saved Excel template: {EXCEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c655e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotation guidelines\n",
    "guidelines = \"\"\"\n",
    "================================================================================\n",
    "ANNOTATION GUIDELINES: Sinhala-English Cyberbullying Dataset Validation\n",
    "================================================================================\n",
    "\n",
    "TASK: Validate translated samples from Hindi-English to Sinhala-English\n",
    "\n",
    "COLUMNS TO FILL:\n",
    "\n",
    "1. Label_Annotator1 / Label_Annotator2:\n",
    "   - 0 = Non-bullying / Positive / Neutral\n",
    "   - 1 = Bullying / Negative / Toxic\n",
    "   \n",
    "2. Intent_Preserved:\n",
    "   - Yes = The translated text conveys the same meaning/intent as original\n",
    "   - No = The meaning was lost or significantly changed\n",
    "   - Unclear = Cannot determine\n",
    "   \n",
    "3. CodeMix_Natural:\n",
    "   - Yes = The Sinhala-English mix sounds natural (as Sri Lankans would speak)\n",
    "   - No = Awkward mixing, over-translated, or unnatural\n",
    "   - Unclear = Cannot determine\n",
    "   \n",
    "4. Bullying_Strength (compared to original):\n",
    "   - Same = Similar intensity of bullying/positivity\n",
    "   - Stronger = Translation is more aggressive/offensive\n",
    "   - Weaker = Translation is milder/less offensive\n",
    "   \n",
    "5. Comments:\n",
    "   - Any observations, issues, or suggestions\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "Good translation (Label: 1, Intent: Yes, CodeMix: Yes, Strength: Same):\n",
    "  Original: \"Tum jaise logon se baat nahi karte.\"\n",
    "  Translated: \"ඔයා වගේ මිනිස්සු එක්ක talk කරන්නේ නෑ.\"\n",
    "  \n",
    "Bad translation (Intent: No, CodeMix: No):\n",
    "  Original: \"You're such a loser yaar.\"\n",
    "  Translated: \"ඔබ පරාජිතයෙක්.\"  (Lost 'loser' and 'yaar', too formal)\n",
    "\n",
    "IMPORTANT NOTES:\n",
    "- Do NOT look at the 'Label' column while annotating (it's the ground truth)\n",
    "- If you're unsure, mark as 'Unclear' and add a comment\n",
    "- Pay special attention to slang and swear words preservation\n",
    "- Natural code-mixing means English words should be kept where Sri Lankans would use them\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Save guidelines\n",
    "GUIDELINES_PATH = '/content/drive/MyDrive/HIN_SIN/annotations/annotation_guidelines.txt'\n",
    "with open(GUIDELINES_PATH, 'w', encoding='utf-8') as f:\n",
    "    f.write(guidelines)\n",
    "print(f\"Saved guidelines: {GUIDELINES_PATH}\")\n",
    "print(guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0bbbe4",
   "metadata": {},
   "source": [
    "---\n",
    "## After Annotation: Calculate Inter-Annotator Agreement\n",
    "\n",
    "Run the cells below after both annotators have completed their annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33442a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load completed annotations\n",
    "# ANNOTATED_PATH = '/content/drive/MyDrive/HIN_SIN/annotations/annotation_completed.csv'\n",
    "# annotated_df = pd.read_csv(ANNOTATED_PATH, encoding='utf-8')\n",
    "\n",
    "# For demonstration, create dummy annotations\n",
    "# REMOVE THIS BLOCK and use actual data\n",
    "annotated_df = annotation_template.copy()\n",
    "# Simulate annotations (REMOVE for real data)\n",
    "annotated_df['Label_Annotator1'] = annotated_df['Label'].apply(lambda x: x if np.random.random() > 0.1 else 1-x)\n",
    "annotated_df['Label_Annotator2'] = annotated_df['Label'].apply(lambda x: x if np.random.random() > 0.15 else 1-x)\n",
    "annotated_df['Intent_Preserved'] = np.random.choice(['Yes', 'No', 'Unclear'], size=len(annotated_df), p=[0.7, 0.2, 0.1])\n",
    "annotated_df['CodeMix_Natural'] = np.random.choice(['Yes', 'No', 'Unclear'], size=len(annotated_df), p=[0.65, 0.25, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inter_annotator_agreement(df):\n",
    "    \"\"\"\n",
    "    Calculate inter-annotator agreement metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Filter out empty/invalid annotations\n",
    "    valid_df = df[\n",
    "        (df['Label_Annotator1'].notna()) & \n",
    "        (df['Label_Annotator2'].notna()) &\n",
    "        (df['Label_Annotator1'] != '') &\n",
    "        (df['Label_Annotator2'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    # Convert to numeric\n",
    "    valid_df['Label_Annotator1'] = pd.to_numeric(valid_df['Label_Annotator1'], errors='coerce')\n",
    "    valid_df['Label_Annotator2'] = pd.to_numeric(valid_df['Label_Annotator2'], errors='coerce')\n",
    "    valid_df = valid_df.dropna(subset=['Label_Annotator1', 'Label_Annotator2'])\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        print(\"No valid annotations found!\")\n",
    "        return None\n",
    "    \n",
    "    # 1. Raw Agreement (% same labels)\n",
    "    agreement = (valid_df['Label_Annotator1'] == valid_df['Label_Annotator2']).mean()\n",
    "    results['raw_agreement'] = agreement\n",
    "    \n",
    "    # 2. Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(\n",
    "        valid_df['Label_Annotator1'].astype(int),\n",
    "        valid_df['Label_Annotator2'].astype(int)\n",
    "    )\n",
    "    results['cohens_kappa'] = kappa\n",
    "    \n",
    "    # 3. Agreement with ground truth\n",
    "    if 'Label' in valid_df.columns:\n",
    "        gt_agree_1 = (valid_df['Label_Annotator1'] == valid_df['Label']).mean()\n",
    "        gt_agree_2 = (valid_df['Label_Annotator2'] == valid_df['Label']).mean()\n",
    "        results['annotator1_vs_gt'] = gt_agree_1\n",
    "        results['annotator2_vs_gt'] = gt_agree_2\n",
    "    \n",
    "    # 4. Sample size\n",
    "    results['n_samples'] = len(valid_df)\n",
    "    \n",
    "    return results, valid_df\n",
    "\n",
    "# Calculate agreement\n",
    "agreement_results, valid_annotations = calculate_inter_annotator_agreement(annotated_df)\n",
    "\n",
    "if agreement_results:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"INTER-ANNOTATOR AGREEMENT RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nSamples analyzed: {agreement_results['n_samples']}\")\n",
    "    print(f\"\\nRaw Agreement: {agreement_results['raw_agreement']:.2%}\")\n",
    "    print(f\"Cohen's Kappa: {agreement_results['cohens_kappa']:.3f}\")\n",
    "    print(f\"\\nAnnotator 1 vs Ground Truth: {agreement_results.get('annotator1_vs_gt', 'N/A'):.2%}\")\n",
    "    print(f\"Annotator 2 vs Ground Truth: {agreement_results.get('annotator2_vs_gt', 'N/A'):.2%}\")\n",
    "    \n",
    "    # Interpret Kappa\n",
    "    kappa = agreement_results['cohens_kappa']\n",
    "    if kappa < 0:\n",
    "        interpretation = \"Poor (worse than chance)\"\n",
    "    elif kappa < 0.2:\n",
    "        interpretation = \"Slight agreement\"\n",
    "    elif kappa < 0.4:\n",
    "        interpretation = \"Fair agreement\"\n",
    "    elif kappa < 0.6:\n",
    "        interpretation = \"Moderate agreement\"\n",
    "    elif kappa < 0.8:\n",
    "        interpretation = \"Substantial agreement\"\n",
    "    else:\n",
    "        interpretation = \"Almost perfect agreement\"\n",
    "    \n",
    "    print(f\"\\nKappa Interpretation: {interpretation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b4df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze qualitative annotations\n",
    "if 'Intent_Preserved' in annotated_df.columns:\n",
    "    print(\"\\n=== Intent Preservation ===\")\n",
    "    print(annotated_df['Intent_Preserved'].value_counts(normalize=True).round(3))\n",
    "\n",
    "if 'CodeMix_Natural' in annotated_df.columns:\n",
    "    print(\"\\n=== Code-Mixing Naturalness ===\")\n",
    "    print(annotated_df['CodeMix_Natural'].value_counts(normalize=True).round(3))\n",
    "\n",
    "if 'Bullying_Strength' in annotated_df.columns:\n",
    "    print(\"\\n=== Bullying Strength Change ===\")\n",
    "    print(annotated_df['Bullying_Strength'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify disagreements for resolution\n",
    "if valid_annotations is not None:\n",
    "    disagreements = valid_annotations[\n",
    "        valid_annotations['Label_Annotator1'] != valid_annotations['Label_Annotator2']\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n=== Disagreements: {len(disagreements)} samples ===\")\n",
    "    \n",
    "    if len(disagreements) > 0:\n",
    "        print(\"\\nSample disagreements:\")\n",
    "        for idx, row in disagreements.head(5).iterrows():\n",
    "            print(f\"\\nID: {row['ID']}\")\n",
    "            if 'Text_SinhalaEnglish' in row:\n",
    "                print(f\"Text: {row['Text_SinhalaEnglish']}\")\n",
    "            elif 'Translated_Text' in row:\n",
    "                print(f\"Text: {row['Translated_Text']}\")\n",
    "            print(f\"A1: {int(row['Label_Annotator1'])}, A2: {int(row['Label_Annotator2'])}, GT: {row['Label']}\")\n",
    "        \n",
    "        # Save disagreements for resolution\n",
    "        disagreements.to_csv(\n",
    "            '/content/drive/MyDrive/HIN_SIN/annotations/disagreements_for_resolution.csv',\n",
    "            index=False, encoding='utf-8'\n",
    "        )\n",
    "        print(f\"\\nSaved disagreements for resolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b456bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final validated sample\n",
    "# Resolve disagreements by majority vote or discussion\n",
    "\n",
    "def resolve_labels(row):\n",
    "    \"\"\"\n",
    "    Resolve label disagreements.\n",
    "    Strategy: Majority vote (with ground truth as tiebreaker)\n",
    "    \"\"\"\n",
    "    a1 = row['Label_Annotator1']\n",
    "    a2 = row['Label_Annotator2']\n",
    "    gt = row['Label']\n",
    "    \n",
    "    if a1 == a2:\n",
    "        return int(a1)\n",
    "    else:\n",
    "        # Use ground truth as tiebreaker\n",
    "        return int(gt)\n",
    "\n",
    "if valid_annotations is not None:\n",
    "    valid_annotations['Final_Label'] = valid_annotations.apply(resolve_labels, axis=1)\n",
    "    \n",
    "    # Save validated sample\n",
    "    valid_annotations.to_csv(\n",
    "        '/content/drive/MyDrive/HIN_SIN/annotations/human_validated_sample.csv',\n",
    "        index=False, encoding='utf-8'\n",
    "    )\n",
    "    print(f\"Saved validated sample: {len(valid_annotations)} samples\")\n",
    "    print(f\"\\nNext step: Run 05_finalization.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
