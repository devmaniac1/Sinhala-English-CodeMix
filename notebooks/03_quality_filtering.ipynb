{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8571ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch pandas numpy tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa411716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'cuda' if device == 0 else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load code-mixed preserved dataset\n",
    "DATASET_PATH = '/content/drive/MyDrive/HIN_SIN/dataset/code_mixed_preserved.csv'\n",
    "# Or for local: DATASET_PATH = '../dataset/code_mixed_preserved.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\\n{df['Label'].value_counts()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c2f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XLM-RoBERTa for zero-shot classification\n",
    "# This model works well for multilingual text including Sinhala\n",
    "\n",
    "print(\"Loading XLM-RoBERTa zero-shot classifier...\")\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",  # Good for zero-shot\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Alternative: Use multilingual model for better Sinhala support\n",
    "# classifier = pipeline(\n",
    "#     \"zero-shot-classification\",\n",
    "#     model=\"joeddav/xlm-roberta-large-xnli\",\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "print(\"Classifier loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classification labels\n",
    "CANDIDATE_LABELS = [\"bullying\", \"not bullying\", \"positive\", \"negative\", \"toxic\", \"friendly\"]\n",
    "\n",
    "# Mapping for bullying detection\n",
    "BULLYING_LABELS = {\"bullying\", \"negative\", \"toxic\"}\n",
    "NON_BULLYING_LABELS = {\"not bullying\", \"positive\", \"friendly\"}\n",
    "\n",
    "def classify_text(text, classifier):\n",
    "    \"\"\"\n",
    "    Classify text using zero-shot classification.\n",
    "    Returns predicted label (0 or 1) and confidence score.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = classifier(text, CANDIDATE_LABELS, multi_label=False)\n",
    "        \n",
    "        # Get top label and score\n",
    "        top_label = result['labels'][0]\n",
    "        top_score = result['scores'][0]\n",
    "        \n",
    "        # Map to binary label\n",
    "        if top_label in BULLYING_LABELS:\n",
    "            predicted_label = 1\n",
    "        else:\n",
    "            predicted_label = 0\n",
    "        \n",
    "        return predicted_label, top_score, top_label\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None, 0.0, str(e)\n",
    "\n",
    "# Test classification\n",
    "test_texts = [\n",
    "    \"You're awesome!\",\n",
    "    \"You're such a loser!\",\n",
    "    \"Thanks for your support bro\"\n",
    "]\n",
    "\n",
    "print(\"Testing classifier:\")\n",
    "for text in test_texts:\n",
    "    label, score, top = classify_text(text, classifier)\n",
    "    print(f\"  '{text}' -> {label} ({top}: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398049fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify original texts (Hindi-English)\n",
    "print(\"Classifying original texts...\")\n",
    "\n",
    "original_predictions = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Original texts\"):\n",
    "    pred_label, confidence, top_label = classify_text(row['Original_Text'], classifier)\n",
    "    original_predictions.append({\n",
    "        'ID': row['ID'],\n",
    "        'original_pred_label': pred_label,\n",
    "        'original_confidence': confidence,\n",
    "        'original_top_label': top_label\n",
    "    })\n",
    "\n",
    "original_pred_df = pd.DataFrame(original_predictions)\n",
    "print(f\"\\nOriginal text predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify translated texts (Sinhala-English)\n",
    "print(\"Classifying translated texts...\")\n",
    "\n",
    "translated_predictions = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Translated texts\"):\n",
    "    pred_label, confidence, top_label = classify_text(row['Translated_Text'], classifier)\n",
    "    translated_predictions.append({\n",
    "        'ID': row['ID'],\n",
    "        'translated_pred_label': pred_label,\n",
    "        'translated_confidence': confidence,\n",
    "        'translated_top_label': top_label\n",
    "    })\n",
    "\n",
    "translated_pred_df = pd.DataFrame(translated_predictions)\n",
    "print(f\"\\nTranslated text predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44eade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictions with original dataframe\n",
    "df = df.merge(original_pred_df, on='ID')\n",
    "df = df.merge(translated_pred_df, on='ID')\n",
    "\n",
    "# Calculate label consistency\n",
    "df['label_match_original'] = (df['original_pred_label'] == df['Label']).astype(int)\n",
    "df['label_match_translated'] = (df['translated_pred_label'] == df['Label']).astype(int)\n",
    "df['label_consistent'] = (df['original_pred_label'] == df['translated_pred_label']).astype(int)\n",
    "\n",
    "print(\"=== Label Consistency Analysis ===\")\n",
    "print(f\"Original matches ground truth: {df['label_match_original'].mean():.2%}\")\n",
    "print(f\"Translated matches ground truth: {df['label_match_translated'].mean():.2%}\")\n",
    "print(f\"Original = Translated (consistency): {df['label_consistent'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec050ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by label\n",
    "print(\"\\n=== Analysis by Ground Truth Label ===\")\n",
    "\n",
    "for label in [0, 1]:\n",
    "    subset = df[df['Label'] == label]\n",
    "    label_name = \"Non-bullying\" if label == 0 else \"Bullying\"\n",
    "    print(f\"\\n{label_name} samples (n={len(subset)}):\")\n",
    "    print(f\"  Original correct: {subset['label_match_original'].mean():.2%}\")\n",
    "    print(f\"  Translated correct: {subset['label_match_translated'].mean():.2%}\")\n",
    "    print(f\"  Consistent: {subset['label_consistent'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49347e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quality filtering criteria\n",
    "def compute_quality_flag(row):\n",
    "    \"\"\"\n",
    "    Compute quality flag based on multiple criteria.\n",
    "    \n",
    "    Returns:\n",
    "    - 'high': Keep with high confidence\n",
    "    - 'medium': Keep but flag for potential review\n",
    "    - 'low': Remove from dataset\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Criterion 1: Translation maintains label (most important)\n",
    "    if row['label_match_translated']:\n",
    "        score += 3\n",
    "    \n",
    "    # Criterion 2: Label consistency between original and translated\n",
    "    if row['label_consistent']:\n",
    "        score += 2\n",
    "    \n",
    "    # Criterion 3: High confidence in translated prediction\n",
    "    if row['translated_confidence'] > 0.7:\n",
    "        score += 1\n",
    "    elif row['translated_confidence'] > 0.5:\n",
    "        score += 0.5\n",
    "    \n",
    "    # Criterion 4: Code-mixing quality score (if available)\n",
    "    if 'quality_score' in row and row['quality_score'] >= 0.6:\n",
    "        score += 1\n",
    "    \n",
    "    # Classify based on total score\n",
    "    if score >= 5:\n",
    "        return 'high'\n",
    "    elif score >= 3:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'low'\n",
    "\n",
    "df['quality_flag'] = df.apply(compute_quality_flag, axis=1)\n",
    "\n",
    "print(\"=== Quality Distribution ===\")\n",
    "print(df['quality_flag'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f80c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View samples from each quality category\n",
    "print(\"\\n=== HIGH QUALITY SAMPLES ===\")\n",
    "high_samples = df[df['quality_flag'] == 'high'].head(5)\n",
    "for _, row in high_samples.iterrows():\n",
    "    print(f\"\\n[Label: {row['Label']}]\")\n",
    "    print(f\"  Original: {row['Original_Text']}\")\n",
    "    print(f\"  Translated: {row['Translated_Text']}\")\n",
    "\n",
    "print(\"\\n=== LOW QUALITY SAMPLES ===\")\n",
    "low_samples = df[df['quality_flag'] == 'low'].head(5)\n",
    "for _, row in low_samples.iterrows():\n",
    "    print(f\"\\n[Label: {row['Label']}]\")\n",
    "    print(f\"  Original: {row['Original_Text']}\")\n",
    "    print(f\"  Translated: {row['Translated_Text']}\")\n",
    "    print(f\"  Pred (orig/trans): {row['original_pred_label']}/{row['translated_pred_label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset - keep high and medium quality samples\n",
    "filtered_df = df[df['quality_flag'].isin(['high', 'medium'])].copy()\n",
    "\n",
    "print(\"=== FILTERING SUMMARY ===\")\n",
    "print(f\"Original samples: {len(df)}\")\n",
    "print(f\"After filtering: {len(filtered_df)}\")\n",
    "print(f\"Removed: {len(df) - len(filtered_df)} ({(len(df) - len(filtered_df))/len(df):.1%})\")\n",
    "\n",
    "print(f\"\\nFiltered label distribution:\")\n",
    "print(filtered_df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4afe335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance after filtering\n",
    "original_balance = df['Label'].value_counts(normalize=True)\n",
    "filtered_balance = filtered_df['Label'].value_counts(normalize=True)\n",
    "\n",
    "print(\"\\n=== Class Balance ===\")\n",
    "print(f\"Original - Non-bullying: {original_balance[0]:.1%}, Bullying: {original_balance[1]:.1%}\")\n",
    "print(f\"Filtered - Non-bullying: {filtered_balance[0]:.1%}, Bullying: {filtered_balance[1]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final filtered dataset\n",
    "output_columns = ['ID', 'Original_Text', 'Translated_Text', 'Label', \n",
    "                  'quality_flag', 'translated_confidence']\n",
    "\n",
    "# Add quality_score if available\n",
    "if 'quality_score' in filtered_df.columns:\n",
    "    output_columns.append('quality_score')\n",
    "\n",
    "final_filtered = filtered_df[output_columns].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "final_filtered = final_filtered.rename(columns={\n",
    "    'Translated_Text': 'Text_SinhalaEnglish',\n",
    "    'Original_Text': 'Text_HindiEnglish'\n",
    "})\n",
    "\n",
    "print(\"Final filtered dataset:\")\n",
    "final_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered dataset\n",
    "OUTPUT_PATH = '/content/drive/MyDrive/HIN_SIN/dataset/quality_filtered.csv'\n",
    "# Or for local: OUTPUT_PATH = '../dataset/quality_filtered.csv'\n",
    "\n",
    "final_filtered.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "print(f\"Saved filtered dataset to: {OUTPUT_PATH}\")\n",
    "\n",
    "# Save removed samples for analysis\n",
    "removed_df = df[df['quality_flag'] == 'low']\n",
    "removed_df.to_csv('/content/drive/MyDrive/HIN_SIN/outputs/removed_samples.csv', \n",
    "                  index=False, encoding='utf-8')\n",
    "print(f\"Saved {len(removed_df)} removed samples for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22416bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUALITY FILTERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nInput: {len(df)} samples\")\n",
    "print(f\"Output: {len(final_filtered)} samples\")\n",
    "print(f\"Removed: {len(df) - len(final_filtered)} samples ({(len(df) - len(final_filtered))/len(df):.1%})\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  Non-bullying (0): {len(final_filtered[final_filtered['Label']==0])}\")\n",
    "print(f\"  Bullying (1): {len(final_filtered[final_filtered['Label']==1])}\")\n",
    "print(f\"\\nNext step: Run 04_validation_prep.ipynb to prepare samples for human validation\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
