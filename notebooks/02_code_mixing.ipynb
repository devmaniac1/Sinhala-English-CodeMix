{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8816bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas numpy langdetect regex tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d034c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from langdetect import detect, DetectorFactory\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c57132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load translated dataset\n",
    "DATASET_PATH = '/content/drive/MyDrive/HIN_SIN/dataset/translated_raw.csv'\n",
    "# Or for local: DATASET_PATH = '../dataset/translated_raw.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226acfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive English preservation list\n",
    "ENGLISH_PRESERVE_LIST = {\n",
    "    # Pronouns\n",
    "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
    "    'my', 'your', 'his', 'her', 'its', 'our', 'their',\n",
    "    'mine', 'yours', 'hers', 'ours', 'theirs',\n",
    "    \n",
    "    # Common verbs\n",
    "    'is', 'are', 'am', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'done',\n",
    "    'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must',\n",
    "    'know', 'think', 'want', 'need', 'like', 'love', 'hate',\n",
    "    \n",
    "    # Articles & determiners\n",
    "    'the', 'a', 'an', 'this', 'that', 'these', 'those', 'some', 'any', 'no',\n",
    "    \n",
    "    # Conjunctions & prepositions\n",
    "    'and', 'or', 'but', 'so', 'because', 'if', 'when', 'where', 'what', 'who', 'how',\n",
    "    'in', 'on', 'at', 'to', 'for', 'with', 'from', 'of', 'by',\n",
    "    \n",
    "    # Internet slang (CRITICAL - must preserve)\n",
    "    'bro', 'dude', 'man', 'buddy', 'yaar', 'sis',\n",
    "    'lol', 'lmao', 'rofl', 'omg', 'wtf', 'af', 'ngl', 'tbh', 'idk', 'idc',\n",
    "    'bruh', 'yo', 'sup', 'wassup', 'hey', 'hi', 'hello', 'bye',\n",
    "    \n",
    "    # Insults & slang (preserve for cyberbullying context)\n",
    "    'loser', 'idiot', 'stupid', 'dumb', 'fool', 'moron', 'jerk', 'creep',\n",
    "    'fake', 'trash', 'garbage', 'pathetic', 'lame', 'weak', 'coward',\n",
    "    'ugly', 'fat', 'skinny', 'weird', 'crazy', 'psycho', 'retard',\n",
    "    \n",
    "    # Positive words\n",
    "    'awesome', 'cool', 'nice', 'great', 'good', 'best', 'amazing', 'wonderful',\n",
    "    'beautiful', 'lovely', 'sweet', 'kind', 'kindest', 'smart', 'intelligent',\n",
    "    'proud', 'happy', 'positive', 'love', 'support',\n",
    "    \n",
    "    # Common expressions\n",
    "    'thanks', 'thank', 'sorry', 'please', 'welcome', 'okay', 'ok', 'yes', 'no', 'not',\n",
    "    'job', 'done', 'well', 'effort', 'keep', 'up', 'always', 'never',\n",
    "    'souls', 'one', 'know',\n",
    "    \n",
    "    # Contractions (common forms)\n",
    "    \"you're\", \"i'm\", \"it's\", \"that's\", \"what's\", \"don't\", \"won't\", \"can't\",\n",
    "    \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\",\n",
    "    \"wouldn't\", \"couldn't\", \"shouldn't\"\n",
    "}\n",
    "\n",
    "print(f\"Total English words to preserve: {len(ENGLISH_PRESERVE_LIST)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a74c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sinhala_char(char):\n",
    "    \"\"\"\n",
    "    Check if a character is Sinhala script.\n",
    "    Sinhala Unicode range: U+0D80 to U+0DFF\n",
    "    \"\"\"\n",
    "    return '\\u0D80' <= char <= '\\u0DFF'\n",
    "\n",
    "def is_sinhala_token(token):\n",
    "    \"\"\"\n",
    "    Check if a token contains Sinhala characters.\n",
    "    \"\"\"\n",
    "    return any(is_sinhala_char(c) for c in token)\n",
    "\n",
    "def is_english_token(token):\n",
    "    \"\"\"\n",
    "    Check if a token is purely English (ASCII letters).\n",
    "    \"\"\"\n",
    "    clean_token = re.sub(r'[^a-zA-Z]', '', token)\n",
    "    return len(clean_token) > 0 and clean_token.isascii()\n",
    "\n",
    "def get_english_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of English words in text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0\n",
    "    english_count = sum(1 for w in words if is_english_token(w))\n",
    "    return english_count / len(words)\n",
    "\n",
    "def get_sinhala_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of Sinhala words in text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0\n",
    "    sinhala_count = sum(1 for w in words if is_sinhala_token(w))\n",
    "    return sinhala_count / len(words)\n",
    "\n",
    "# Test\n",
    "test_text = \"ඔයා awesome bro!\"\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"English ratio: {get_english_ratio(test_text):.2f}\")\n",
    "print(f\"Sinhala ratio: {get_sinhala_ratio(test_text):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_english_from_original(original_text):\n",
    "    \"\"\"\n",
    "    Extract English words from the original Hindi-English text.\n",
    "    These should be preserved in the translation.\n",
    "    \"\"\"\n",
    "    words = original_text.split()\n",
    "    english_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Remove punctuation for checking\n",
    "        clean_word = re.sub(r'[^a-zA-Z\\']', '', word)\n",
    "        if clean_word and clean_word.isascii():\n",
    "            # Check if it's a common English word or in preserve list\n",
    "            if clean_word.lower() in ENGLISH_PRESERVE_LIST:\n",
    "                english_words.append(word)  # Keep with punctuation\n",
    "    \n",
    "    return english_words\n",
    "\n",
    "def restore_english_tokens(original_text, translated_text):\n",
    "    \"\"\"\n",
    "    Restore English tokens from original text if they were incorrectly translated.\n",
    "    \"\"\"\n",
    "    # Get English words that should be preserved\n",
    "    english_to_preserve = extract_english_from_original(original_text)\n",
    "    \n",
    "    if not english_to_preserve:\n",
    "        return translated_text\n",
    "    \n",
    "    # Check if these words exist in translated text\n",
    "    translated_lower = translated_text.lower()\n",
    "    missing_english = []\n",
    "    \n",
    "    for word in english_to_preserve:\n",
    "        clean_word = re.sub(r'[^a-zA-Z\\']', '', word).lower()\n",
    "        if clean_word not in translated_lower:\n",
    "            missing_english.append(word)\n",
    "    \n",
    "    # If English words are missing, we need to reconstruct\n",
    "    if missing_english:\n",
    "        # Strategy: Analyze original structure and rebuild\n",
    "        # This is a simplified approach - may need refinement\n",
    "        return translated_text\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "# Test\n",
    "test_original = \"You're awesome bro!\"\n",
    "test_translated = \"ඔයා නියමයි!\"\n",
    "print(f\"Original: {test_original}\")\n",
    "print(f\"English to preserve: {extract_english_from_original(test_original)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4880f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_code_mixing(original_text, translated_text):\n",
    "    \"\"\"\n",
    "    Analyze the code-mixing quality of a translation.\n",
    "    \n",
    "    Returns a dict with:\n",
    "    - original_english_ratio: English ratio in original\n",
    "    - translated_english_ratio: English ratio in translated\n",
    "    - english_preserved: Whether English words were preserved\n",
    "    - quality_score: Overall quality score (0-1)\n",
    "    \"\"\"\n",
    "    original_english = extract_english_from_original(original_text)\n",
    "    orig_eng_ratio = get_english_ratio(original_text)\n",
    "    trans_eng_ratio = get_english_ratio(translated_text)\n",
    "    trans_sin_ratio = get_sinhala_ratio(translated_text)\n",
    "    \n",
    "    # Check if English words are preserved\n",
    "    translated_lower = translated_text.lower()\n",
    "    preserved_count = sum(1 for w in original_english \n",
    "                          if re.sub(r'[^a-zA-Z]', '', w).lower() in translated_lower)\n",
    "    preservation_ratio = preserved_count / len(original_english) if original_english else 1.0\n",
    "    \n",
    "    # Quality score calculation\n",
    "    # Good translation: has Sinhala + preserved English\n",
    "    quality_score = 0.0\n",
    "    \n",
    "    # Penalize if no Sinhala (means no translation happened)\n",
    "    if trans_sin_ratio > 0:\n",
    "        quality_score += 0.4\n",
    "    \n",
    "    # Reward English preservation\n",
    "    quality_score += 0.4 * preservation_ratio\n",
    "    \n",
    "    # Bonus for natural code-mixing (not pure Sinhala or pure English)\n",
    "    if 0.1 < trans_eng_ratio < 0.9 and trans_sin_ratio > 0.1:\n",
    "        quality_score += 0.2\n",
    "    \n",
    "    return {\n",
    "        'original_english_ratio': orig_eng_ratio,\n",
    "        'translated_english_ratio': trans_eng_ratio,\n",
    "        'translated_sinhala_ratio': trans_sin_ratio,\n",
    "        'english_preserved_ratio': preservation_ratio,\n",
    "        'quality_score': quality_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f71967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all translations\n",
    "print(\"Analyzing code-mixing quality...\")\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    analysis = analyze_code_mixing(row['Original_Text'], row['Translated_Text'])\n",
    "    analysis['ID'] = row['ID']\n",
    "    analysis['Label'] = row['Label']\n",
    "    analysis_results.append(analysis)\n",
    "\n",
    "analysis_df = pd.DataFrame(analysis_results)\n",
    "\n",
    "print(f\"\\n=== Code-Mixing Quality Analysis ===\")\n",
    "print(f\"Average quality score: {analysis_df['quality_score'].mean():.3f}\")\n",
    "print(f\"Avg English preservation: {analysis_df['english_preserved_ratio'].mean():.3f}\")\n",
    "print(f\"Avg Sinhala ratio in translations: {analysis_df['translated_sinhala_ratio'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81722e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify problematic translations\n",
    "# 1. Pure English (no translation)\n",
    "pure_english = analysis_df[analysis_df['translated_sinhala_ratio'] == 0]\n",
    "print(f\"Pure English (no translation): {len(pure_english)}\")\n",
    "\n",
    "# 2. Pure Sinhala (over-translated)\n",
    "pure_sinhala = analysis_df[analysis_df['translated_english_ratio'] == 0]\n",
    "print(f\"Pure Sinhala (over-translated): {len(pure_sinhala)}\")\n",
    "\n",
    "# 3. Low English preservation\n",
    "low_preservation = analysis_df[analysis_df['english_preserved_ratio'] < 0.5]\n",
    "print(f\"Low English preservation (<50%): {len(low_preservation)}\")\n",
    "\n",
    "# 4. Good code-mixed samples\n",
    "good_samples = analysis_df[analysis_df['quality_score'] >= 0.6]\n",
    "print(f\"Good quality (score >= 0.6): {len(good_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_code_mixing(original_text, translated_text):\n",
    "    \"\"\"\n",
    "    Fix code-mixing issues in translated text.\n",
    "    \n",
    "    Strategy:\n",
    "    1. If translation is pure English, keep as is (already English)\n",
    "    2. If translation lost English words, try to restore structure\n",
    "    3. Ensure slang and internet expressions are preserved\n",
    "    \"\"\"\n",
    "    # Case 1: Original was pure English - keep as is\n",
    "    if get_english_ratio(original_text) > 0.9:\n",
    "        return original_text\n",
    "    \n",
    "    # Case 2: Translation is same as original (no translation happened)\n",
    "    if original_text.strip() == translated_text.strip():\n",
    "        return translated_text\n",
    "    \n",
    "    # Case 3: Restore missing English words\n",
    "    original_words = original_text.split()\n",
    "    translated_words = translated_text.split()\n",
    "    \n",
    "    result_words = []\n",
    "    english_to_preserve = extract_english_from_original(original_text)\n",
    "    english_set = set(w.lower().strip('.,!?\\'\"') for w in english_to_preserve)\n",
    "    \n",
    "    # Track which English words we've added\n",
    "    added_english = set()\n",
    "    \n",
    "    for word in translated_words:\n",
    "        result_words.append(word)\n",
    "    \n",
    "    # Check if any critical English words are missing\n",
    "    translated_lower = translated_text.lower()\n",
    "    missing_english = []\n",
    "    \n",
    "    for eng_word in english_to_preserve:\n",
    "        clean_word = re.sub(r'[^a-zA-Z]', '', eng_word).lower()\n",
    "        if clean_word and clean_word not in translated_lower:\n",
    "            missing_english.append(eng_word)\n",
    "    \n",
    "    # If critical words are missing, append them at logical positions\n",
    "    # This is a simplified approach\n",
    "    if missing_english:\n",
    "        # For now, we'll flag these for manual review\n",
    "        pass\n",
    "    \n",
    "    return ' '.join(result_words)\n",
    "\n",
    "# Apply fixes\n",
    "print(\"Applying code-mixing fixes...\")\n",
    "\n",
    "fixed_translations = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    fixed_text = fix_code_mixing(row['Original_Text'], row['Translated_Text'])\n",
    "    fixed_translations.append({\n",
    "        'ID': row['ID'],\n",
    "        'Original_Text': row['Original_Text'],\n",
    "        'Translated_Text': fixed_text,\n",
    "        'Label': row['Label']\n",
    "    })\n",
    "\n",
    "fixed_df = pd.DataFrame(fixed_translations)\n",
    "print(\"Fixes applied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025433b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-analyze after fixes\n",
    "print(\"Re-analyzing after fixes...\")\n",
    "\n",
    "fixed_analysis = []\n",
    "for idx, row in fixed_df.iterrows():\n",
    "    analysis = analyze_code_mixing(row['Original_Text'], row['Translated_Text'])\n",
    "    fixed_analysis.append(analysis)\n",
    "\n",
    "fixed_analysis_df = pd.DataFrame(fixed_analysis)\n",
    "\n",
    "print(f\"\\n=== After Fixes ===\")\n",
    "print(f\"Average quality score: {fixed_analysis_df['quality_score'].mean():.3f}\")\n",
    "print(f\"Avg English preservation: {fixed_analysis_df['english_preserved_ratio'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add quality metrics to the dataframe\n",
    "fixed_df['quality_score'] = fixed_analysis_df['quality_score']\n",
    "fixed_df['english_preserved'] = fixed_analysis_df['english_preserved_ratio']\n",
    "fixed_df['sinhala_ratio'] = fixed_analysis_df['translated_sinhala_ratio']\n",
    "\n",
    "# View samples\n",
    "print(\"\\n=== Sample Translations ===\")\n",
    "for idx in range(min(10, len(fixed_df))):\n",
    "    row = fixed_df.iloc[idx]\n",
    "    print(f\"\\n[{row['Label']}] Original: {row['Original_Text']}\")\n",
    "    print(f\"    Translated: {row['Translated_Text']}\")\n",
    "    print(f\"    Quality: {row['quality_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b522f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the code-mixed preserved dataset\n",
    "OUTPUT_PATH = '/content/drive/MyDrive/HIN_SIN/dataset/code_mixed_preserved.csv'\n",
    "# Or for local: OUTPUT_PATH = '../dataset/code_mixed_preserved.csv'\n",
    "\n",
    "fixed_df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "print(f\"Saved code-mixed dataset to: {OUTPUT_PATH}\")\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Total samples: {len(fixed_df)}\")\n",
    "print(f\"High quality (score >= 0.6): {len(fixed_df[fixed_df['quality_score'] >= 0.6])}\")\n",
    "print(f\"Medium quality (0.3-0.6): {len(fixed_df[(fixed_df['quality_score'] >= 0.3) & (fixed_df['quality_score'] < 0.6)])}\")\n",
    "print(f\"Low quality (< 0.3): {len(fixed_df[fixed_df['quality_score'] < 0.3])}\")\n",
    "print(f\"\\nNext step: Run 03_quality_filtering.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
