{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch sentencepiece pandas tqdm langdetect\n",
    "!pip install -q git+https://github.com/AI4Bharat/IndicTrans2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from langdetect import detect, DetectorFactory\n",
    "import torch\n",
    "\n",
    "# Set seed for reproducibility\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Check GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Hindi-English dataset\n",
    "# Update path based on your setup\n",
    "DATASET_PATH = '/content/drive/MyDrive/HIN_SIN/dataset/HindiEnglish.csv'\n",
    "# Or for local: DATASET_PATH = '../dataset/HindiEnglish.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\\n{df['Label'].value_counts()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize IndicTrans2 for Hindi to Sinhala translation\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# IndicTrans2 model for Indic-to-Indic translation\n",
    "MODEL_NAME = \"ai4bharat/indictrans2-indic-indic-1B\"\n",
    "\n",
    "print(\"Loading IndicTrans2 model... (this may take a few minutes)\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, trust_remote_code=True).to(device)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89715209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language detection helper functions\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect the language of a text.\n",
    "    Returns 'en' for English, 'hi' for Hindi, or 'unknown'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "def is_english_token(token):\n",
    "    \"\"\"\n",
    "    Check if a token is English based on character set and common patterns.\n",
    "    \"\"\"\n",
    "    # Check if token contains only ASCII letters\n",
    "    if re.match(r'^[a-zA-Z]+$', token):\n",
    "        return True\n",
    "    # Check for common English patterns with punctuation\n",
    "    if re.match(r\"^[a-zA-Z]+[!',.?]*$\", token):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_hindi_token(token):\n",
    "    \"\"\"\n",
    "    Check if token contains Devanagari script (Hindi).\n",
    "    \"\"\"\n",
    "    # Devanagari Unicode range: U+0900 to U+097F\n",
    "    devanagari_pattern = re.compile(r'[\\u0900-\\u097F]')\n",
    "    return bool(devanagari_pattern.search(token))\n",
    "\n",
    "def is_romanized_hindi(token):\n",
    "    \"\"\"\n",
    "    Check if token is likely Romanized Hindi (Hinglish).\n",
    "    This is heuristic-based.\n",
    "    \"\"\"\n",
    "    # Common Hindi words written in Roman script\n",
    "    common_hindi_words = {\n",
    "        'tum', 'kya', 'hai', 'nahi', 'karte', 'ho', 'kahi', 'ka', 'ki', 'ke',\n",
    "        'tera', 'teri', 'mera', 'meri', 'tumhara', 'tumhari', 'uska', 'uski',\n",
    "        'bohot', 'bahut', 'accha', 'acha', 'badiya', 'kaam', 'baat', 'log',\n",
    "        'logon', 'jaise', 'kaise', 'pagal', 'bakwas', 'band', 'kar', 'samjhta',\n",
    "        'samajh', 'aukaat', 'sharam', 'aati', 'dekhne', 'layak', 'face',\n",
    "        'nautanki', 'dimag', 'kharab', 'irritating', 'sakta', 'itna', 'idiot',\n",
    "        'presentation', 'tha', 'bhai', 'kamaal', 'diya', 'tujhe', 'gaya'\n",
    "    }\n",
    "    return token.lower() in common_hindi_words\n",
    "\n",
    "# Test the functions\n",
    "test_tokens = ['You', 'awesome', 'Tum', 'kya', 'hai', 'bhai', '!', 'yaar']\n",
    "for token in test_tokens:\n",
    "    print(f\"{token}: English={is_english_token(token)}, Hindi={is_romanized_hindi(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f361b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define English words/slang to preserve (NOT translate)\n",
    "PRESERVE_ENGLISH = {\n",
    "    # Common English words in code-mixed text\n",
    "    'you', 'your', 'i', 'me', 'my', 'we', 'us', 'they', 'them', 'he', 'she', 'it',\n",
    "    'is', 'are', 'am', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',\n",
    "    'the', 'a', 'an', 'this', 'that', 'these', 'those',\n",
    "    'and', 'or', 'but', 'so', 'because', 'if', 'when', 'where', 'what', 'who', 'how',\n",
    "    'not', 'no', 'yes', 'ok', 'okay',\n",
    "    \n",
    "    # Internet slang & expressions (MUST preserve)\n",
    "    'bro', 'dude', 'man', 'buddy', 'yaar', 'lol', 'omg', 'wtf', 'lmao', 'rofl',\n",
    "    'fake', 'loser', 'idiot', 'stupid', 'dumb', 'fool', 'jerk', 'moron',\n",
    "    'awesome', 'cool', 'nice', 'great', 'good', 'bad', 'worst', 'best',\n",
    "    'thanks', 'sorry', 'please', 'welcome',\n",
    "    'love', 'hate', 'like', 'proud', 'happy', 'sad',\n",
    "    \n",
    "    # Common expressions\n",
    "    \"you're\", \"i'm\", \"it's\", \"that's\", \"what's\", \"don't\", \"won't\", \"can't\",\n",
    "    'job', 'done', 'well', 'effort', 'support', 'positive', 'kindest', 'souls', 'know',\n",
    "    'keep', 'up', 'always'\n",
    "}\n",
    "\n",
    "print(f\"Total English words to preserve: {len(PRESERVE_ENGLISH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3aed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_hindi_to_sinhala(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Translate Hindi text to Sinhala using IndicTrans2.\n",
    "    \"\"\"\n",
    "    # Prepare input with language tags\n",
    "    # Format: <2si> for target language (Sinhala)\n",
    "    input_text = f\"<2si> {text}\"\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    translated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated\n",
    "\n",
    "# Test translation\n",
    "test_hindi = \"तुम बहुत अच्छा काम करते हो\"\n",
    "print(f\"Hindi: {test_hindi}\")\n",
    "print(f\"Sinhala: {translate_hindi_to_sinhala(test_hindi, tokenizer, model, device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_code_mixed_sentence(sentence, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Translate a Hindi-English code-mixed sentence to Sinhala-English.\n",
    "    Preserves English tokens and translates only Hindi portions.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Tokenize the sentence\n",
    "    2. Identify English vs Hindi tokens\n",
    "    3. Group consecutive Hindi tokens\n",
    "    4. Translate Hindi groups to Sinhala\n",
    "    5. Reconstruct with preserved English\n",
    "    \"\"\"\n",
    "    # Handle pure English sentences\n",
    "    if all(c.isascii() for c in sentence.replace(' ', '')):\n",
    "        # Check if it's mostly English\n",
    "        words = sentence.split()\n",
    "        english_count = sum(1 for w in words if is_english_token(w.strip('.,!?')))\n",
    "        if english_count / len(words) > 0.7:  # >70% English\n",
    "            return sentence  # Keep as is\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = sentence.split()\n",
    "    result_tokens = []\n",
    "    hindi_buffer = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        clean_token = token.strip('.,!?\\'\"')\n",
    "        punctuation = token[len(clean_token):] if len(token) > len(clean_token) else ''\n",
    "        \n",
    "        # Check if token should be preserved (English or in preserve list)\n",
    "        if (is_english_token(clean_token) and \n",
    "            clean_token.lower() in PRESERVE_ENGLISH):\n",
    "            # Flush Hindi buffer first\n",
    "            if hindi_buffer:\n",
    "                hindi_text = ' '.join(hindi_buffer)\n",
    "                try:\n",
    "                    sinhala_text = translate_hindi_to_sinhala(hindi_text, tokenizer, model, device)\n",
    "                    result_tokens.append(sinhala_text)\n",
    "                except:\n",
    "                    result_tokens.append(hindi_text)  # Fallback\n",
    "                hindi_buffer = []\n",
    "            result_tokens.append(token)  # Preserve English\n",
    "        \n",
    "        elif is_hindi_token(clean_token) or is_romanized_hindi(clean_token):\n",
    "            # Add to Hindi buffer for batch translation\n",
    "            hindi_buffer.append(token)\n",
    "        \n",
    "        else:\n",
    "            # Unknown - try to translate\n",
    "            hindi_buffer.append(token)\n",
    "    \n",
    "    # Flush remaining Hindi buffer\n",
    "    if hindi_buffer:\n",
    "        hindi_text = ' '.join(hindi_buffer)\n",
    "        try:\n",
    "            sinhala_text = translate_hindi_to_sinhala(hindi_text, tokenizer, model, device)\n",
    "            result_tokens.append(sinhala_text)\n",
    "        except:\n",
    "            result_tokens.append(hindi_text)\n",
    "    \n",
    "    return ' '.join(result_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d4a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample sentences from the dataset\n",
    "test_sentences = [\n",
    "    \"You're awesome!\",\n",
    "    \"Tum jaise logon se baat nahi karte.\",\n",
    "    \"Thanks for your support yaar.\",\n",
    "    \"Teri aukaat kya hai samjhta hai?\",\n",
    "    \"Idiot kahi ka!\",\n",
    "    \"Good job, well done!\",\n",
    "    \"Bakwas band kar.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSLATION TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    translated = translate_code_mixed_sentence(sent, tokenizer, model, device)\n",
    "    print(f\"\\nOriginal: {sent}\")\n",
    "    print(f\"Translated: {translated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8248fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the entire dataset\n",
    "print(f\"Processing {len(df)} sentences...\")\n",
    "\n",
    "translated_texts = []\n",
    "errors = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Translating\"):\n",
    "    try:\n",
    "        original_text = row['Text']\n",
    "        translated_text = translate_code_mixed_sentence(\n",
    "            original_text, tokenizer, model, device\n",
    "        )\n",
    "        translated_texts.append({\n",
    "            'ID': row['ID'],\n",
    "            'Original_Text': original_text,\n",
    "            'Translated_Text': translated_text,\n",
    "            'Label': row['Label']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        errors.append({'ID': row['ID'], 'Text': row['Text'], 'Error': str(e)})\n",
    "        translated_texts.append({\n",
    "            'ID': row['ID'],\n",
    "            'Original_Text': row['Text'],\n",
    "            'Translated_Text': row['Text'],  # Keep original on error\n",
    "            'Label': row['Label']\n",
    "        })\n",
    "    \n",
    "    # Save checkpoint every 100 samples\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        checkpoint_df = pd.DataFrame(translated_texts)\n",
    "        checkpoint_df.to_csv('/content/drive/MyDrive/HIN_SIN/outputs/translation_checkpoint.csv', \n",
    "                            index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nCompleted! Errors: {len(errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output DataFrame\n",
    "translated_df = pd.DataFrame(translated_texts)\n",
    "\n",
    "print(f\"\\nTranslated Dataset Shape: {translated_df.shape}\")\n",
    "print(f\"\\nSample translations:\")\n",
    "translated_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the translated dataset\n",
    "OUTPUT_PATH = '/content/drive/MyDrive/HIN_SIN/dataset/translated_raw.csv'\n",
    "# Or for local: OUTPUT_PATH = '../dataset/translated_raw.csv'\n",
    "\n",
    "translated_df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "print(f\"Saved translated dataset to: {OUTPUT_PATH}\")\n",
    "\n",
    "# Save errors log if any\n",
    "if errors:\n",
    "    errors_df = pd.DataFrame(errors)\n",
    "    errors_df.to_csv('/content/drive/MyDrive/HIN_SIN/outputs/translation_errors.csv', \n",
    "                     index=False, encoding='utf-8')\n",
    "    print(f\"Saved {len(errors)} errors to translation_errors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be764ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSLATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(translated_df)}\")\n",
    "print(f\"Successful translations: {len(translated_df) - len(errors)}\")\n",
    "print(f\"Errors: {len(errors)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(translated_df['Label'].value_counts())\n",
    "print(f\"\\nNext step: Run 02_code_mixing.ipynb to refine code-mixing preservation\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
